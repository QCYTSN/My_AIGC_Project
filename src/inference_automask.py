import torch
from diffusers import StableDiffusionPipeline, StableDiffusionInpaintPipeline
# ğŸš¨ ä¿®æ­£ç‚¹ 1: è¿™é‡Œçš„ç±»åä¿®æ­£ä¸º OwlViT...
from transformers import OwlViTProcessor, OwlViTForObjectDetection
from PIL import Image, ImageDraw
import numpy as np
import os

# === é…ç½® ===
base_model = "./sd-v1-5"
lora_dog = "./outputs/lora_dog"
lora_glass = "./outputs/lora_sunglasses"
save_dir = "results/automask_experiment"
os.makedirs(save_dir, exist_ok=True)

device = "cuda"

# === 1. åˆå§‹åŒ–æ¨¡å‹ç¾¤ ===
print(">>> ğŸ§  åˆå§‹åŒ–æ¨¡å‹ç¾¤...")

# A. ç”Ÿæˆæ¨¡å‹ (T2I)
pipe_t2i = StableDiffusionPipeline.from_pretrained(
    base_model, torch_dtype=torch.float16, safety_checker=None
).to(device)
pipe_t2i.load_lora_weights(lora_dog, adapter_name="dog")
pipe_t2i.set_adapters(["dog"], adapter_weights=[1.0])

# B. æ„ŸçŸ¥æ¨¡å‹ (OWL-ViT)
# æŒ‡å‘åˆšæ‰ä¸‹è½½å¥½çš„æœ¬åœ°æ–‡ä»¶å¤¹
local_owl_path = "./models/owlvit-base-patch32" 

print(f">>> ğŸ“‚ ä»æœ¬åœ°åŠ è½½æ„ŸçŸ¥æ¨¡å‹: {local_owl_path} ...")
processor = OwlViTProcessor.from_pretrained(local_owl_path)
model_owl = OwlViTForObjectDetection.from_pretrained(local_owl_path).to(device)

# === 2. ç”Ÿæˆåº•å›¾ ===
print(">>> ğŸ¶ Step 1: ç”Ÿæˆåº•å›¾...")
# ç¨å¾®æŠŠ prompt æ”¹å¾—ç®€å•ç‚¹ï¼Œç¡®ä¿èƒ½ç”»å‡ºæ­£è„¸ï¼Œæé«˜æ£€æµ‹æˆåŠŸç‡
prompt_dog = "a photo of a sks dog sitting, front view, looking at camera, high quality"
# å›ºå®š Seed æ–¹ä¾¿è°ƒè¯•
generator = torch.Generator(device).manual_seed(1024)
image_dog = pipe_t2i(prompt_dog, num_inference_steps=30, generator=generator).images[0]
image_dog.save(f"{save_dir}/base_dog.png")

del pipe_t2i
torch.cuda.empty_cache()

# === 3. æ™ºèƒ½æ„ŸçŸ¥ä¸å‡ ä½•è®¡ç®— ===
print(">>> ğŸ‘ï¸ Step 2: è§†è§‰æ„ŸçŸ¥ä¸Maskè®¡ç®—...")

# OWL-ViT éœ€è¦æ–‡æœ¬æç¤ºæ¥æ‰¾ç‰©ä½“
texts = [["eyes", "face"]]
inputs = processor(text=texts, images=image_dog, return_tensors="pt").to(device)

with torch.no_grad():
    outputs = model_owl(**inputs)

target_sizes = torch.Tensor([image_dog.size[::-1]]).to(device)
# é™ä½ä¸€ç‚¹é˜ˆå€¼ (0.05) ç¡®ä¿èƒ½æŠŠçœ¼ç›æ‰¾å‡ºæ¥
results = processor.post_process_object_detection(outputs, threshold=0.05, target_sizes=target_sizes)[0]

boxes = results["boxes"].cpu().numpy()
labels = results["labels"].cpu().numpy()

eye_boxes = []
for box, label in zip(boxes, labels):
    if label == 0: # label 0 is "eyes"
        eye_boxes.append(box)

# --- å‡ ä½•ç®—æ³• ---
mask = Image.new("L", (512, 512), 0)
draw = ImageDraw.Draw(mask)

if len(eye_boxes) >= 1:
    print(f"âœ… æ£€æµ‹åˆ° {len(eye_boxes)} åªçœ¼ç›ï¼")
    
    x1 = np.min([b[0] for b in eye_boxes])
    y1 = np.min([b[1] for b in eye_boxes])
    x2 = np.max([b[2] for b in eye_boxes])
    y2 = np.max([b[3] for b in eye_boxes])
    
    w = x2 - x1
    h = y2 - y1
    
    # ç¨å¾®è°ƒæ•´ä¸€ä¸‹æ‰©å¼ ç³»æ•°ï¼Œé˜²æ­¢ç”»å¤ªå¤§äº†
    pad_w = w * 0.3 
    pad_h = h * 0.5 
    
    final_box = [
        max(0, x1 - pad_w), 
        max(0, y1 - pad_h), 
        min(512, x2 + pad_w), 
        min(512, y2 + pad_h * 0.5)
    ]
    
    draw.rectangle(final_box, fill=255)
    print(f"    Mask åŒºåŸŸ: {final_box}")
else:
    print("âš ï¸ æœªæ£€æµ‹åˆ°çœ¼ç›ï¼ä½¿ç”¨å…œåº•ç­–ç•¥...")
    draw.rectangle([160, 160, 352, 220], fill=255)

mask.save(f"{save_dir}/auto_mask.png")

# === 4. å±€éƒ¨é‡ç»˜ ===
print(">>> ğŸ•¶ï¸ Step 3: æ³¨å…¥å¢¨é•œæ¦‚å¿µ...")

pipe_inpaint = StableDiffusionInpaintPipeline.from_pretrained(
    base_model, torch_dtype=torch.float16, safety_checker=None
).to(device)

pipe_inpaint.load_lora_weights(lora_glass, adapter_name="sunglasses")
pipe_inpaint.set_adapters(["sunglasses"], adapter_weights=[1.0])

final_image = pipe_inpaint(
    prompt="a photo of a sks sunglasses on a dog face, black frame, transparent glass",
    negative_prompt="eyes, fur, messy",
    image=image_dog,
    mask_image=mask,
    strength=0.9,
    num_inference_steps=40
).images[0]

final_image.save(f"{save_dir}/final_result.png")
print(f"ğŸ‰ é—­ç¯å®Œæˆï¼ç»“æœä¿å­˜åœ¨ {save_dir}/final_result.png")