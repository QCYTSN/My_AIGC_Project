import torch
import torch.nn.functional as F
from diffusers import StableDiffusionPipeline
from diffusers.models.attention_processor import AttnProcessor2_0
from typing import Optional, Union, List, Dict, Any

class MaskedCrossAttentionProcessor:
    """
    âš”ï¸ [ç»ˆæä¿®å¤ç‰ˆ] å¸¦ FP32 å®‰å…¨é” + ç»´åº¦ä¿®æ­£çš„ Mask Attention å¤„ç†å™¨
    """
    def __init__(self, token_idx_to_mask: Dict[int, torch.Tensor], scale: float = 1.0):
        self.token_idx_to_mask = token_idx_to_mask
        self.scale = scale

    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None, temb=None):
        # === 1. è·å– Q, K, V ===
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        
        query = attn.to_q(hidden_states)

        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)

        # è½¬æ¢åˆ°å¤šå¤´ç»´åº¦ [Batch*Heads, SeqLen, Dim]
        query = attn.head_to_batch_dim(query)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)

        # === ğŸš¨ å…³é”®ä¿®å¤åŒº ===
        # 1. è½¬ FP32 é˜²æ­¢æº¢å‡º
        original_dtype = query.dtype
        query = query.to(torch.float32)
        key = key.to(torch.float32)

        # 2. è®¡ç®— Attention Scores
        # âš ï¸ ä¹‹å‰çš„é”™è¯¯ï¼šè¿™é‡Œå¤šåŒ…äº†ä¸€å±‚ attn.batch_to_head_dimï¼Œå¯¼è‡´ç»´åº¦è¢«å‹ç¼©ï¼Œç°å·²åˆ é™¤
        attention_scores = torch.baddbmm(
            torch.empty(
                query.shape[0],
                query.shape[1],
                key.shape[1],
                dtype=query.dtype,
                device=query.device,
            ),
            query,
            key.transpose(-1, -2),
            beta=0,
            alpha=attn.scale,
        )
        
        # === 2. æ³¨å…¥ Mask æ§åˆ¶ ===
        current_res = int(sequence_length ** 0.5) 
        
        for token_idx, mask in self.token_idx_to_mask.items():
            # ç¼©æ”¾ Mask
            resized_mask = F.interpolate(mask, size=(current_res, current_res), mode="nearest")
            flat_mask = resized_mask.reshape(1, -1)
            
            # åˆ¶é€ æƒ©ç½šé¡¹ (Mask ä¸º 0 çš„åœ°æ–¹æ‰£ 10000 åˆ†)
            # FP32 ä¸‹éå¸¸å®‰å…¨
            penalty = (1 - flat_mask) * -50.0
            
            # æ–½åŠ æƒ©ç½š
            attention_scores[:, :, token_idx] = attention_scores[:, :, token_idx] + penalty.to(attention_scores.device)

        # === 3. æ”¶å°¾ ===
        # Softmax (FP32)
        attention_probs = attention_scores.softmax(dim=-1)
        
        # è½¬å› FP16 (å¦‚æœåŸæ¨¡å‹æ˜¯FP16)
        attention_probs = attention_probs.to(original_dtype)
        
        # è®¡ç®—è¾“å‡º
        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        
        # çº¿æ€§æŠ•å°„è¾“å‡º
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)

        return hidden_states

class MaskStableDiffusionPipeline(StableDiffusionPipeline):
    def __call__(
        self,
        prompt: Union[str, List[str]] = None,
        mask_config: Dict[str, torch.Tensor] = None, 
        **kwargs,
    ):
        if mask_config is not None:
            print(f">>> ğŸ›¡ï¸ æ­£åœ¨æ³¨å…¥ Mask æ§åˆ¶: {list(mask_config.keys())}")
            self.register_attention_control(prompt, mask_config)
        
        return super().__call__(prompt=prompt, **kwargs)

    def register_attention_control(self, prompt, mask_config):
        # è·å– Token ID
        input_ids = self.tokenizer(prompt).input_ids
        token_idx_to_mask = {}
        
        print(">>> ğŸ” Token æ˜ å°„:")
        words = prompt.split()
        for word, mask in mask_config.items():
            try:
                found = False
                for i, token_id in enumerate(input_ids):
                    decoded_word = self.tokenizer.decode([token_id]).strip()
                    # ç®€å•åŒ¹é…é€»è¾‘
                    if word in decoded_word:
                        print(f"    - '{word}' å¯¹åº” Token ID: {token_id} (ä½ç½® {i})")
                        token_idx_to_mask[i] = mask.to(self.device, dtype=torch.float32)
                        found = True
                if not found:
                     print(f"âš ï¸ è­¦å‘Š: Prompt ä¸­æœªæ‰¾åˆ°å•è¯ '{word}' çš„ Token")
            except ValueError:
                pass

        # æ›¿æ¢å¤„ç†å™¨
        attn_procs = {}
        for name in self.unet.attn_processors.keys():
            if name.endswith("attn2.processor"): 
                attn_procs[name] = MaskedCrossAttentionProcessor(token_idx_to_mask)
            else:
                attn_procs[name] = AttnProcessor2_0()
        
        self.unet.set_attn_processor(attn_procs)