import torch
from diffusers import StableDiffusionPipeline
from diffusers.models.attention_processor import AttnProcessor
import json
import numpy as np
from PIL import Image, ImageDraw
import os
import cv2

# === é…ç½® ===
device = "cuda"
model_path = "./sd-v1-5"
json_source_dir = "results/baseline_eval_v5/cat_hat/json"
output_dir = "results/research_week3_debug"
os.makedirs(output_dir, exist_ok=True)

# === 1. å¤„ç†å™¨ V2.4 (å«å¢å¼ºé€»è¾‘) ===
class SpatialGateAttnProcessorV2_4:
    def __init__(self, target_token_ids, bbox, width=512, height=512):
        self.target_token_ids = target_token_ids
        self.bbox = bbox
        self.W = width
        self.H = height
        self.debug_saved = False 
        self.trigger_count = 0

    def __call__(
        self,
        attn,
        hidden_states,
        encoder_hidden_states=None,
        attention_mask=None,
        temb=None,
        *args, **kwargs
    ):
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        query = attn.to_q(hidden_states)

        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)

        query = attn.head_to_batch_dim(query)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)

        attention_probs = attn.get_attention_scores(query, key, attention_mask)
        
        # --- ğŸ’‰ æ‰‹æœ¯å¼€å§‹ ---
        spatial_pixels = attention_probs.shape[1]
        spatial_res = int(np.sqrt(spatial_pixels))
        
        # åªè¦åˆ†è¾¨ç‡æ˜¯ 16 æˆ– 32ï¼Œå°±æ‰§è¡Œæ§åˆ¶
        if spatial_res in [16, 32]:
            self.trigger_count += 1
            
            mask = torch.zeros((spatial_res, spatial_res), device=attention_probs.device)
            scale_x = spatial_res / self.W
            scale_y = spatial_res / self.H
            
            x1 = int(self.bbox[0] * scale_x)
            y1 = int(self.bbox[1] * scale_y)
            x2 = int(self.bbox[2] * scale_x)
            y2 = int(self.bbox[3] * scale_y)
            
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(spatial_res, x2), min(spatial_res, y2)
            
            # 1. åˆ¶ä½œ Mask
            mask[y1:y2, x1:x2] = 1.0
            
            # Debug: æ‰“å°ä¸€æ¬¡ç¡®è®¤
            if not self.debug_saved and spatial_res == 16:
                print(f"ğŸ”¥ DEBUG: æ‹¦æˆª {spatial_res}x{spatial_res} å±‚ | æ¡†å†…åŒºåŸŸ: x[{x1}:{x2}] y[{y1}:{y2}]")
                self.debug_saved = True

            mask_flat = mask.view(1, -1, 1) # [1, Pixels, 1]
            
            # 2. æ ¸å¿ƒæ§åˆ¶é€»è¾‘ (å¢å¼º + æŠ‘åˆ¶)
            for token_id in self.target_token_ids:
                current_probs = attention_probs[:, :, token_id]
                
                # A. æŠ‘åˆ¶æ¡†å¤–: ä¹˜ä»¥ 0 (Hard Gating)
                masked_probs = current_probs * mask_flat.squeeze()
                
                # B. å¢å¼ºæ¡†å†…: ä¹˜ä»¥ 5.0 (Amplification)
                # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†è§£å†³â€œçŒ«æ²¡æœ‰å¸½å­â€çš„é—®é¢˜ï¼Œå¼ºè¿«å®ƒåœ¨æ¡†å†…æ¿€æ´»
                amplified_probs = masked_probs * 5.0
                
                attention_probs[:, :, token_id] = amplified_probs

        # --- æ‰‹æœ¯ç»“æŸ ---

        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)

        return hidden_states

# === 2. æ³¨å†Œå·¥å…· (ä¿®å¤åŒ¹é…é€»è¾‘) ===
def register_spatial_control(pipe, target_words, bbox):
    tokenizer = pipe.tokenizer
    prompt = "a photo of a cute cat wearing a red hat" 
    tokens = tokenizer.encode(prompt)
    decoder = tokenizer.decode
    
    target_ids = []
    print(f"\nğŸ” Token åŒ¹é…:")
    for idx, token in enumerate(tokens):
        decoded = decoder([token]).strip().lower()
        for word in target_words:
            if word.lower() in decoded:
                print(f"   âœ… ID {idx} -> '{decoded}'")
                target_ids.append(idx)
    
    custom_processor = SpatialGateAttnProcessorV2_4(target_ids, bbox)
    
    from diffusers.models.attention_processor import AttnProcessor
    default_processor = AttnProcessor()

    attn_procs = {}
    hook_count = 0
    
    # ğŸš¨ å…³é”®ä¿®å¤ï¼šéå†æ‰€æœ‰å¤„ç†å™¨ï¼Œåªè¦åå­—å« attn2 å°±æ›¿æ¢
    print("\nğŸ› ï¸ å¼€å§‹æŒ‚è½½å¤„ç†å™¨...")
    for name in pipe.unet.attn_processors.keys():
        if "attn2" in name:
            attn_procs[name] = custom_processor
            hook_count += 1
            # åªæ‰“å°å‰3ä¸ªä½œä¸ºç¤ºä¾‹ï¼Œé¿å…åˆ·å±
            if hook_count <= 3:
                print(f"   ğŸ”— æŒ‚è½½åˆ°: {name}")
        else:
            attn_procs[name] = default_processor
            
    pipe.unet.set_attn_processor(attn_procs)
    print(f"ğŸ”Œ æœ€ç»ˆç»“æœ: å·²æˆåŠŸæŒ‚è½½æ§åˆ¶å™¨åˆ° {hook_count} ä¸ª Cross-Attention å±‚")
    
    if hook_count == 0:
        print("âŒ ä¸¥é‡é”™è¯¯ï¼šä¾ç„¶æ²¡æœ‰æŒ‚è½½åˆ°ä»»ä½•å±‚ï¼è¯·æ£€æŸ¥ Key åç§°ã€‚")
        # æ‰“å°æ‰€æœ‰ Key ä¾›è°ƒè¯•
        print(list(pipe.unet.attn_processors.keys())[:5])
        
    return custom_processor

# === 3. ä¸»ç¨‹åº ===
if __name__ == "__main__":
    pipe = StableDiffusionPipeline.from_pretrained(
        model_path, torch_dtype=torch.float16, safety_checker=None
    ).to(device)
    
    # è¯»å–ç¬¬ä¸€å¼  JSON
    json_files = sorted(os.listdir(json_source_dir))
    target_json = json_files[0] 
    with open(f"{json_source_dir}/{target_json}", 'r') as f:
        meta = json.load(f)
    
    seed = meta['seed']
    bbox = meta['target_bbox'] 
    
    # ç›®æ ‡è¯
    target_words = ["red", "hat"] 
    prompt = "a photo of a cute cat wearing a red hat"
    
    # A. Baseline
    print("\nğŸ§ª ç”Ÿæˆ Baseline...")
    pipe.unet.set_default_attn_processor()
    generator = torch.Generator(device).manual_seed(seed)
    img_baseline = pipe(prompt, num_inference_steps=30, generator=generator).images[0]
    img_baseline.save(f"{output_dir}/compare_baseline.png")
    
    # B. Ours
    print(f"\nğŸ’‰ æ³¨å…¥æ§åˆ¶ (æŠ‘åˆ¶å¤–éƒ¨ + å¢å¼ºå†…éƒ¨)...")
    processor = register_spatial_control(pipe, target_words, bbox)
    
    generator = torch.Generator(device).manual_seed(seed)
    img_ours = pipe(prompt, num_inference_steps=30, generator=generator).images[0]
    img_ours.save(f"{output_dir}/compare_ours_v2_4.png")
    
    print(f"\nğŸ“Š ç»Ÿè®¡: æ§åˆ¶é€»è¾‘è§¦å‘äº† {processor.trigger_count} æ¬¡")
    
    # C. ç”»æ¡†
    img_vis = img_baseline.copy()
    draw = ImageDraw.Draw(img_vis)
    draw.rectangle(bbox, outline="green", width=5)
    img_vis.save(f"{output_dir}/vis_target_box.png")

    print(f"ğŸ‰ ä¿®å¤å®Œæˆï¼è¯·æ£€æŸ¥ {output_dir}")