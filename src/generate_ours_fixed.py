import torch
from diffusers import StableDiffusionPipeline
from diffusers.models.attention_processor import AttnProcessor
import json
import numpy as np
from PIL import Image
import os

# === âš™ï¸ é…ç½®åŒºåŸŸ ===
device = "cuda"
model_path = "./sd-v1-5"

# è¾“å…¥è·¯å¾„ï¼šç¡®ä¿åŒæ—¶åŒ…å«çŒ«å’Œç‹—
input_dirs = [
    "results/baseline_eval_v5/cat_hat/json",
    "results/baseline_eval_v5/dog_scarf/json", 
]

# è¾“å‡ºè·¯å¾„ (ä¾ç„¶è¦†ç›–è¿™ä¸ªæ–‡ä»¶å¤¹)
output_root = "results/final_comparison_fixed"
os.makedirs(output_root, exist_ok=True)

# === 1. æ ¸å¿ƒå¤„ç†å™¨ (å›é€€åˆ°æ¸©å’Œå¢å¼ºç‰ˆ V2.4) ===
class SpatialGateAttnProcessor_Balanced:
    def __init__(self, target_token_ids, bbox, width=512, height=512):
        self.target_token_ids = target_token_ids
        self.bbox = bbox
        self.W = width
        self.H = height
        
    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None, temb=None, *args, **kwargs):
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None: encoder_hidden_states = hidden_states
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        query = attn.head_to_batch_dim(query)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)
        attention_probs = attn.get_attention_scores(query, key, attention_mask)
        
        # --- ğŸ’‰ æ§åˆ¶é€»è¾‘ ---
        spatial_pixels = attention_probs.shape[1]
        spatial_res = int(np.sqrt(spatial_pixels))
        
        # é’ˆå¯¹ 16x16 (256) å’Œ 32x32 (1024) è¿›è¡Œæ§åˆ¶
        if spatial_res in [16, 32]:
            # 1. åˆ¶ä½œ Mask
            mask = torch.zeros((spatial_res, spatial_res), device=attention_probs.device)
            scale_x, scale_y = spatial_res / self.W, spatial_res / self.H
            
            x1 = int(self.bbox[0] * scale_x)
            y1 = int(self.bbox[1] * scale_y)
            x2 = int(self.bbox[2] * scale_x)
            y2 = int(self.bbox[3] * scale_y)
            
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(spatial_res, x2), min(spatial_res, y2)
            
            mask[y1:y2, x1:x2] = 1.0
            mask_flat = mask.view(1, -1, 1) # [1, Pixels, 1]
            
            # 2. æ¸©å’Œæ§åˆ¶
            for token_id in self.target_token_ids:
                current_map = attention_probs[:, :, token_id]
                
                # A. æŠ‘åˆ¶å¤–éƒ¨: ä¹˜ 0 (Hard Gating) - è¿™ä¸ªä¿ç•™ï¼Œé˜²æ­¢èƒŒæ™¯æ³„éœ²
                masked_map = current_map * mask_flat.squeeze()
                
                # B. æ¸©å’Œå¢å¼ºå†…éƒ¨: ä¹˜ 5.0 (Balanced Boosting)
                # ğŸ’¡ å›é€€ç‚¹ï¼šä» 20.0 æ”¹å› 5.0ï¼Œè®©çŒ«æ˜¾å½¢
                attention_probs[:, :, token_id] = masked_map * 5.0 

        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        return hidden_states

# === 2. åŠ¨æ€ Prompt ç®¡ç† (ä¿ç•™ä¹‹å‰çš„ä¼˜åŒ–ï¼Œç”¨æ›´å…·ä½“çš„è¯) ===
def get_prompt(category):
    if category == "hat": 
        return "a photo of a cute cat wearing a red knitted beanie"
    
    if category == "scarf": 
        return "a photo of a dog wearing a blue winter scarf"
        
    return ""

def get_target_words(category):
    if category == "hat": 
        return ["red", "knitted", "beanie"]
        
    if category == "scarf": 
        return ["blue", "scarf"]
        
    return []

# === 3. æ‰¹é‡æ‰§è¡Œé€»è¾‘ ===
def run_batch():
    print(">>> ğŸš€ å¯åŠ¨å¹³è¡¡ç‰ˆ (x5.0) ç”Ÿæˆè„šæœ¬ (çŒ«+ç‹—)...")
    pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16, safety_checker=None).to(device)
    
    for json_dir in input_dirs:
        if not os.path.exists(json_dir): continue
        task_name = json_dir.split("/")[-2] 
        save_path = f"{output_root}/{task_name}"
        os.makedirs(save_path, exist_ok=True)
        
        print(f"\nğŸ“‚ å¤„ç†ä»»åŠ¡ç»„: {task_name}")
        
        files = sorted([f for f in os.listdir(json_dir) if f.endswith(".json")])
        for f_name in files:
            with open(f"{json_dir}/{f_name}", 'r') as f:
                meta = json.load(f)
            
            seed = meta['seed']
            bbox = meta['target_bbox']
            category = meta['category']
            
            prompt = get_prompt(category)
            target_words = get_target_words(category)
            
            tokenizer = pipe.tokenizer
            tokens = tokenizer.encode(prompt)
            decoder = tokenizer.decode
            
            target_ids = []
            for idx, token in enumerate(tokens):
                decoded = decoder([token]).strip().lower()
                for word in target_words:
                    if word.lower() in decoded:
                        target_ids.append(idx)
            
            if not target_ids:
                print(f"âš ï¸ Warning: æ²¡æ‰¾åˆ°ç›®æ ‡è¯ Tokens! (Prompt: {prompt})")
            
            # æŒ‚è½½å¹³è¡¡ç‰ˆå¤„ç†å™¨
            processor = SpatialGateAttnProcessor_Balanced(target_ids, bbox)
            attn_procs = {}
            for name in pipe.unet.attn_processors.keys():
                if "attn2" in name: 
                    attn_procs[name] = processor
                else: 
                    attn_procs[name] = AttnProcessor()
            pipe.unet.set_attn_processor(attn_procs)
            
            generator = torch.Generator(device).manual_seed(seed)
            image = pipe(prompt, num_inference_steps=30, generator=generator).images[0]
            
            img_name = f_name.replace(".json", ".png")
            image.save(f"{save_path}/{img_name}")
            print(f"   âœ… ç”Ÿæˆ: {img_name}")

if __name__ == "__main__":
    run_batch()