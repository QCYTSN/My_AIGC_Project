import torch
from diffusers import StableDiffusionPipeline
import numpy as np
from PIL import Image
import cv2
import os
import matplotlib.pyplot as plt

# === é…ç½® ===
device = "cuda"
model_path = "./sd-v1-5" 
output_dir = "results/research_week2"
os.makedirs(output_dir, exist_ok=True)

# === æ ¸å¿ƒé»‘ç§‘æŠ€ï¼šAttention é’©å­ ===
class AttentionStore:
    def __init__(self):
        self.step_store = {} 

    def __call__(self, attn, is_cross: bool, place_in_unet: str):
        if not is_cross:
            return
        
        # æˆ‘ä»¬å¯»æ‰¾ 16x16 = 256 åƒç´ çš„å±‚
        pixels = attn.shape[1]
        if pixels == 16 ** 2: 
            key = f"{place_in_unet}_{pixels}"
            if key not in self.step_store:
                self.step_store[key] = []
            self.step_store[key].append(attn)

    def reset(self):
        self.step_store = {}

# âœ… ç¨³å¥çš„æ³¨å†Œæ–¹æ³•
def register_attention_control(pipe, controller):
    def ca_forward(self, place_in_unet):
        def forward(hidden_states, encoder_hidden_states=None, attention_mask=None, **kwargs):
            is_cross = encoder_hidden_states is not None
            
            query = self.to_q(hidden_states)
            if encoder_hidden_states is None:
                encoder_hidden_states = hidden_states
            
            key = self.to_k(encoder_hidden_states)
            value = self.to_v(encoder_hidden_states)

            query = self.head_to_batch_dim(query)
            key = self.head_to_batch_dim(key)
            value = self.head_to_batch_dim(value)

            attention_probs = self.get_attention_scores(query, key, attention_mask)
            
            # ğŸš¨ å·å– Attention Map
            controller(attention_probs, is_cross, place_in_unet)
            
            hidden_states = torch.bmm(attention_probs, value)
            hidden_states = self.batch_to_head_dim(hidden_states)
            hidden_states = self.to_out[0](hidden_states)
            hidden_states = self.to_out[1](hidden_states)
            return hidden_states
            
        return forward

    print(">>> æ­£åœ¨ç»™ UNet å®‰è£…æ¢é’ˆ...")
    hook_count = 0
    for name, module in pipe.unet.named_modules():
        if name.endswith("attn2"):
            if "down" in name: place = "down"
            elif "mid" in name: place = "mid"
            elif "up" in name: place = "up"
            else: continue
            
            if hasattr(module, "to_q"):
                module.forward = ca_forward(module, place)
                hook_count += 1
    
    if hook_count == 0:
        print("âŒ è­¦å‘Šï¼šæ²¡æœ‰æŒ‚è½½åˆ°ä»»ä½• Attention å±‚ï¼è¯·æ£€æŸ¥æ¨¡å‹ç»“æ„ã€‚")
    else:
        print(f"âœ… æˆåŠŸæŒ‚è½½äº† {hook_count} ä¸ª Attention å±‚ã€‚")

# === å¯è§†åŒ–å·¥å…· (ä¿®å¤äº† float16 é—®é¢˜) ===
def visualize_attention(pipe, prompt, target_word, seed=42):
    print(f"\nğŸ‘€ æ­£åœ¨æ¢æµ‹ Prompt: '{prompt}' ä¸­å•è¯ '{target_word}' çš„æ³¨æ„åŠ›...")
    
    controller = AttentionStore()
    register_attention_control(pipe, controller)
    
    generator = torch.Generator(device).manual_seed(seed)
    image = pipe(prompt, num_inference_steps=30, generator=generator).images[0]
    image.save(f"{output_dir}/vis_base_{target_word}.png")
    
    # èšåˆ Attention Maps
    attention_maps = []
    for key in controller.step_store:
        attn = torch.cat(controller.step_store[key], dim=0) 
        # åªåœ¨ batch/heads ç»´åº¦å¹³å‡ï¼Œä¿ç•™ Pixels ç»´åº¦
        attn = attn.mean(0) 
        attention_maps.append(attn)
    
    if not attention_maps:
        print("âŒ ä¾ç„¶æ²¡æœ‰æ•è·åˆ° Attention Mapã€‚è¿™å¾ˆå¥‡æ€ªã€‚")
        return

    # å †å æ‰€æœ‰å±‚çš„ map å¹¶å–å¹³å‡
    global_attn = torch.stack(attention_maps).mean(0) 
    
    # æ‰¾åˆ°ç›®æ ‡å•è¯çš„ Token ID
    tokenizer = pipe.tokenizer
    tokens = tokenizer.encode(prompt)
    decoder = tokenizer.decode
    
    target_idx = -1
    print(f"Tokenåˆ—è¡¨: {[decoder([t]) for t in tokens]}")
    
    for idx, token in enumerate(tokens):
        decoded = decoder([token]).strip().lower()
        if target_word.lower() in decoded:
            target_idx = idx
            break
            
    if target_idx == -1:
        print(f"âŒ è­¦å‘Šï¼šæ²¡æ‰¾åˆ°å•è¯ '{target_word}' çš„ tokenã€‚")
        return

    print(f"âœ… é”å®š Token ID: {target_idx} ('{decoder([tokens[target_idx]])}')")
    
    # æå–çƒ­åŠ›å›¾
    # ğŸš¨ğŸš¨ğŸš¨ ã€ä¿®å¤æ ¸å¿ƒã€‘å¼ºåˆ¶è½¬ä¸º float32ï¼Œé˜²æ­¢ OpenCV æŠ¥é”™ ğŸš¨ğŸš¨ğŸš¨
    attn_map = global_attn[:, target_idx].reshape(16, 16).cpu().numpy().astype(np.float32)
    
    # æ¸²æŸ“
    attn_heatmap = cv2.resize(attn_map, (512, 512))
    # å½’ä¸€åŒ–
    attn_heatmap = (attn_heatmap - attn_heatmap.min()) / (attn_heatmap.max() - attn_heatmap.min())
    
    # ç”»å›¾
    fig, axs = plt.subplots(1, 2, figsize=(12, 6))
    axs[0].imshow(image)
    axs[0].set_title("Generated Image")
    axs[0].axis("off")
    
    axs[1].imshow(image)
    axs[1].imshow(attn_heatmap, cmap='jet', alpha=0.6) 
    axs[1].set_title(f"Attention: {target_word}")
    axs[1].axis("off")
    
    save_path = f"{output_dir}/vis_heatmap_{target_word}.png"
    plt.savefig(save_path, bbox_inches='tight')
    print(f"ğŸ‰ å¯è§†åŒ–å®Œæˆï¼š{save_path}")

# === ä¸»ç¨‹åº ===
if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹
    pipe = StableDiffusionPipeline.from_pretrained(
        model_path, torch_dtype=torch.float16, safety_checker=None
    ).to(device)
    
    # å®éªŒ 1ï¼šå¸½å­
    prompt = "a photo of a cute cat wearing a red hat"
    visualize_attention(pipe, prompt, "hat", seed=2024)
    
    # å®éªŒ 2ï¼šå›´å·¾
    prompt_scarf = "a photo of a dog wearing a blue scarf"
    visualize_attention(pipe, prompt_scarf, "scarf", seed=2024)