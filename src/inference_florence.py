import os
import torch
import numpy as np
from PIL import Image, ImageDraw, ImageFilter
from diffusers import StableDiffusionPipeline, StableDiffusionInpaintPipeline
from transformers import AutoProcessor, AutoModelForCausalLM

# === âš™ï¸ å…¨å±€é…ç½® ===
device = "cuda"
base_model = "./sd-v1-5"
lora_dog = "./outputs/lora_dog"
lora_glass = "./outputs/lora_sunglasses"

# ç»§ç»­ä½¿ç”¨ Large-FT
florence_model_path = "./models/Florence-2-large-ft"

save_dir = "results/florence_final"
os.makedirs(save_dir, exist_ok=True)

# === 1. åŠ è½½æ¨¡å‹ ===
print(">>> ğŸ§  åŠ è½½æ¨¡å‹ç¾¤ (Final Version)...")

# A. Florence-2
processor = AutoProcessor.from_pretrained(florence_model_path, trust_remote_code=True)
model_florence = AutoModelForCausalLM.from_pretrained(
    florence_model_path, 
    torch_dtype=torch.float16, 
    trust_remote_code=True
).to(device)

# B. SD T2I
pipe_t2i = StableDiffusionPipeline.from_pretrained(
    base_model, torch_dtype=torch.float16, safety_checker=None
).to(device)
pipe_t2i.load_lora_weights(lora_dog, adapter_name="dog")

print(">>> âœ… æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")

# === 2. Step 1: ç”Ÿæˆåº•å›¾ ===
print(">>> ğŸ¶ Step 1: ç”Ÿæˆåº•å›¾...")
prompt_dog = "a photo of a sks dog sitting, front view, looking at camera, high quality, 8k"
# å›ºå®š Seed
generator = torch.Generator(device).manual_seed(2024) 

image_dog = pipe_t2i(prompt_dog, num_inference_steps=30, generator=generator).images[0]
image_path = f"{save_dir}/step1_base.png"
image_dog.save(image_path)

del pipe_t2i
torch.cuda.empty_cache()

# === 3. Step 2: Florence-2 å¯»æ‰¾å¤´éƒ¨ ===
print(">>> ğŸ‘ï¸ Step 2: å¯»æ‰¾å¤´éƒ¨å®šä½...")

task_prompt = "<REFERRING_EXPRESSION_SEGMENTATION>"
text_input = "head" # ç»§ç»­æ‰¾å¤´ï¼Œå› ä¸ºå¤´æœ€å‡†

inputs = processor(text=task_prompt + text_input, images=image_dog, return_tensors="pt").to(device, torch.float16)

# use_cache=False ä¿æŒå…¼å®¹æ€§
generated_ids = model_florence.generate(
    input_ids=inputs["input_ids"],
    pixel_values=inputs["pixel_values"],
    max_new_tokens=1024,
    do_sample=False,
    num_beams=3,
    use_cache=False 
)

generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
prediction = processor.post_process_generation(
    generated_text, 
    task=task_prompt, 
    image_size=(image_dog.width, image_dog.height)
)
segmentation_results = prediction[task_prompt]

print(f"âœ… æ£€æµ‹ç›®æ ‡: {segmentation_results['labels']}")

# === 4. ç»˜åˆ¶â€œä½ç½—é¢å…·â€ Mask (æ ¸å¿ƒé€»è¾‘ä¿®æ”¹) ===
# åˆ›å»ºå…¨é»‘ç”»å¸ƒ
mask = Image.new("L", image_dog.size, 0)
draw = ImageDraw.Draw(mask)

# æˆ‘ä»¬åªç”¨ Florence-2 çš„ç»“æœæ¥è®¡ç®—ä½ç½®ï¼Œä¸å†ç›´æ¥ç”¨å®ƒçš„å¤šè¾¹å½¢
# å…ˆç”»ä¸€ä¸ªä¸´æ—¶çš„ Mask æ¥è·å– bbox
temp_mask = Image.new("L", image_dog.size, 0)
temp_draw = ImageDraw.Draw(temp_mask)

for polygon in segmentation_results["polygons"]:
    points = np.array(polygon).reshape(-1, 2)
    points_tuple = [tuple(pt) for pt in points]
    temp_draw.polygon(points_tuple, fill=255)

# è·å–å¤´éƒ¨çš„è¾¹ç•Œæ¡† (Left, Top, Right, Bottom)
bbox = temp_mask.getbbox()

if bbox:
    left, top, right, bottom = bbox
    width = right - left
    height = bottom - top
    
    print(f"ğŸ“ å¤´éƒ¨ä½ç½®: x={left}, y={top}, w={width}, h={height}")

    # --------- ğŸ¨ æ ¸å¿ƒé­”æ³•ï¼šç»˜åˆ¶â€œä½ç½—é¢å…·â€ ---------
    # æˆ‘ä»¬ä¸ä¿¡ä»» Florence çš„çœ¼ç›æ£€æµ‹ï¼Œæˆ‘ä»¬ä¿¡ä»»â€œæ¯”ä¾‹å­¦â€
    # åœ¨ç‹—çš„å¤´éƒ¨ä¸­ï¼Œçœ¼ç›é€šå¸¸ä½äºé«˜åº¦çš„ 30% åˆ° 55% ä¹‹é—´
    
    # 1. è®¾å®šå¢¨é•œåŒºåŸŸçš„ä¸Šè¾¹ç•Œ (é¿å¼€é¢å¤´)
    eye_top = top + height * 0.30 
    
    # 2. è®¾å®šå¢¨é•œåŒºåŸŸçš„ä¸‹è¾¹ç•Œ (é¿å¼€é¼»å­)
    eye_bottom = top + height * 0.55
    
    # 3. è®¾å®šå·¦å³è¾¹ç•Œ (ç¨å¾®å¾€é‡Œæ”¶ä¸€ç‚¹ï¼Œé¿å¼€è€³æœµæ ¹éƒ¨)
    eye_left = left + width * 0.15
    eye_right = right - width * 0.15
    
    # 4. ç»˜åˆ¶ä¸€ä¸ªåœ†è§’çŸ©å½¢ (æ›´åƒå¢¨é•œçš„å½¢çŠ¶ï¼Œç»™ SD æ›´å¥½çš„æš—ç¤º)
    # è¿™ç§æ¨ªæ¡å½¢çŠ¶ä¼šå¼ºè¿« SD ç”Ÿæˆç±»ä¼¼ aviator æˆ– wayfarer çš„å½¢çŠ¶
    draw.rounded_rectangle(
        [(eye_left, eye_top), (eye_right, eye_bottom)], 
        radius=15, 
        fill=255
    )
    print("ğŸ•¶ï¸ å·²ç”Ÿæˆâ€œä½ç½—é¢å…·â€Maskï¼šåŸºäºå¤´éƒ¨æ¯”ä¾‹æ¨ç®—çœ¼ç›åŒºåŸŸã€‚")
    
else:
    print("âš ï¸ æœªæ£€æµ‹åˆ°å¤´éƒ¨ï¼Œä½¿ç”¨å…¨é»‘ Mask (å°†å¯¼è‡´å¤±è´¥)ã€‚")

# é€‚åº¦è†¨èƒ€ï¼Œè®©è¾¹ç¼˜èåˆæ›´å¥½
mask = mask.filter(ImageFilter.MaxFilter(9))
# é«˜æ–¯æ¨¡ç³Šï¼Œè®©è¾¹ç•Œä¸è¦å¤ªç”Ÿç¡¬
mask = mask.filter(ImageFilter.GaussianBlur(radius=3))

mask.save(f"{save_dir}/step2_zorro_mask.png")

del model_florence
torch.cuda.empty_cache()

# === 5. Step 3: Inpainting ===
print(">>> ğŸ•¶ï¸ Step 3: ä½©æˆ´å¢¨é•œ...")

pipe_inpaint = StableDiffusionInpaintPipeline.from_pretrained(
    base_model, torch_dtype=torch.float16, safety_checker=None
).to(device)
pipe_inpaint.load_lora_weights(lora_glass, adapter_name="sunglasses")

final_image = pipe_inpaint(
    prompt="a photo of a sks sunglasses on dog eyes, black frame, dark lenses, highly detailed, professional photography",
    # è´Ÿé¢æç¤ºè¯éå¸¸é‡è¦
    negative_prompt="forehead, ears, nose, fur texture on glass, ugly, messy, distorted frame",
    image=image_dog,
    mask_image=mask,
    strength=1.0, 
    num_inference_steps=50 # ç¨å¾®å¢åŠ æ­¥æ•°æå‡è´¨æ„Ÿ
).images[0]

final_image.save(f"{save_dir}/final_result.png")
print(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼ç»“æœä¿å­˜åœ¨ {save_dir}/final_result.png")